{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz761cYnc9zFyhyPOD7FE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anonymisedaccount/gametheorycoursework2/blob/main/EntropyScores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install libraries\n",
        "!pip install transformers torch --quiet\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model=BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "#Defining a function to calculate the average entropy\n",
        "def averageentropyscore(text):\n",
        "    idsinput=tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "    entropyscores=[]\n",
        "\n",
        "    for i in range(1, len(idsinput)-1):\n",
        "        maskedinput=idsinput.clone()\n",
        "        maskedinput[i]=tokenizer.mask_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs=model(maskedinput.unsqueeze(0))\n",
        "            logits=outputs.logits[0, i]\n",
        "            probs=torch.softmax(logits, dim=0)\n",
        "            entropy=-torch.sum(probs * torch.log(probs+ 1e-12)).item()\n",
        "            entropyscores.append(entropy)\n",
        "\n",
        "    averageentropy=sum(entropyscores)/len(entropyscores)\n",
        "    return round(averageentropy,4)\n",
        "\n",
        "#Inputting the text\n",
        "text= \"\"\"\n",
        "In fact, Descartes anticipates objections that appeal to qualities like solidity or texture in Meditations II, particularly through the wax example. He observes that the wax, when placed near a flame, loses all of its sensory properties — its hardness, shape, even its sound — and yet remains recognisably the same substance. What remains constant, he argues, is its capacity to be extended, figured, and moved. This suggests that extension, unlike solidity, survives changes in external form. For Descartes, this is evidence that extension is grasped not through the senses but by the intellect alone. Therefore, he could respond to the solidity objection by arguing that resistance or impenetrability are contingent, whereas extension is essential. As such, the essence of body lies not in how it feels or behaves, but in its fundamental dimensionality — which alone persists through change.\n",
        "\"\"\"\n",
        "\n",
        "#Calculating and printing the average entropy scores\n",
        "originality=averageentropyscore(text)\n",
        "print(originality)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff0oUsNgObq0",
        "outputId": "34e2e9ca-703e-4d6b-9d22-9ee17db37e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.342\n"
          ]
        }
      ]
    }
  ]
}